{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\narendran.thesma\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMPANY</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>DISTRIBUTOR</th>\n",
       "      <th>ARTICLE ID</th>\n",
       "      <th>MODEL NUMBER</th>\n",
       "      <th>ARTICLE NAME</th>\n",
       "      <th>SUBBRAND</th>\n",
       "      <th>SPORTS CATEGORY</th>\n",
       "      <th>PRODUCT DIVISION</th>\n",
       "      <th>PRODUCT GROUP</th>\n",
       "      <th>...</th>\n",
       "      <th>CURRENT PRICE IN EUR</th>\n",
       "      <th>INITIAL PRICE IN SELECTED CURRENCY</th>\n",
       "      <th>CURRENT PRICE IN SELECTED CURRENCY</th>\n",
       "      <th>SELECTED CURRENCY</th>\n",
       "      <th>PRODUCT INTRODUCTION DATE</th>\n",
       "      <th>DISCOUNTED SINCE</th>\n",
       "      <th>PRODUCT EXIT DATE</th>\n",
       "      <th>PRODUCT DESCRIPTION</th>\n",
       "      <th>PRODUCT URL</th>\n",
       "      <th>IMAGE-SERVER URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Own eCom</td>\n",
       "      <td>BG0387-001</td>\n",
       "      <td>BG0387</td>\n",
       "      <td>ナイキ コア ハーフ KV ゴルフバッグ</td>\n",
       "      <td>Performance</td>\n",
       "      <td>Golf</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Bags</td>\n",
       "      <td>...</td>\n",
       "      <td>99.144</td>\n",
       "      <td>116.640</td>\n",
       "      <td>99.144</td>\n",
       "      <td>EUR</td>\n",
       "      <td>2/22/2016</td>\n",
       "      <td>12/26/2016</td>\n",
       "      <td>1/2/2017</td>\n",
       "      <td>整理しやすいゴルフバッグ。快適な持ち運び。 ナイキ コア ハーフ KV ゴルフバッグは、専用...</td>\n",
       "      <td>http://store.nike.com/jp/ja_jp/pd/%25E3%2583%2...</td>\n",
       "      <td>http://usporamap287.am.adsint.biz/zoomimages/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Own eCom</td>\n",
       "      <td>839240-001</td>\n",
       "      <td>839240</td>\n",
       "      <td>ナイキ コルテッツ QS キッズシューズ</td>\n",
       "      <td>Sport Inspired</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>Sport Inspired Footwear</td>\n",
       "      <td>...</td>\n",
       "      <td>62.820</td>\n",
       "      <td>82.620</td>\n",
       "      <td>62.820</td>\n",
       "      <td>EUR</td>\n",
       "      <td>2/22/2016</td>\n",
       "      <td>4/5/2016</td>\n",
       "      <td>7/26/2016</td>\n",
       "      <td>高級感のあるレトロスタイル   ナイキ コルテッツ QS キッズシューズは、上質なレザーのア...</td>\n",
       "      <td>http://store.nike.com/jp/ja_jp/pd/%25E3%2583%2...</td>\n",
       "      <td>http://usporamap287.am.adsint.biz/zoomimages/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Own eCom</td>\n",
       "      <td>GL0783-101</td>\n",
       "      <td>GL0783</td>\n",
       "      <td>ナイキ レジン スピード レッド ゴルフボール</td>\n",
       "      <td>Performance</td>\n",
       "      <td>Golf</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Sport Equipment</td>\n",
       "      <td>...</td>\n",
       "      <td>31.104</td>\n",
       "      <td>31.104</td>\n",
       "      <td>31.104</td>\n",
       "      <td>EUR</td>\n",
       "      <td>2/15/2016</td>\n",
       "      <td>Not discounted yet</td>\n",
       "      <td>8/29/2016</td>\n",
       "      <td>高初速でより遠くへ ナイキ レジン スピード レッド ゴルフボールは、更にソフトになった新開...</td>\n",
       "      <td>http://store.nike.com/jp/ja_jp/pd/%25E3%2583%2...</td>\n",
       "      <td>http://usporamap287.am.adsint.biz/zoomimages/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Own eCom</td>\n",
       "      <td>GL0781-101</td>\n",
       "      <td>GL0781</td>\n",
       "      <td>ナイキ レジン ツアー ブラック ゴルフボール</td>\n",
       "      <td>Performance</td>\n",
       "      <td>Golf</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Sport Equipment</td>\n",
       "      <td>...</td>\n",
       "      <td>28.764</td>\n",
       "      <td>58.320</td>\n",
       "      <td>28.764</td>\n",
       "      <td>EUR</td>\n",
       "      <td>2/15/2016</td>\n",
       "      <td>2/27/2017</td>\n",
       "      <td>6/27/2017</td>\n",
       "      <td>低スピンでより遠くへ ナイキ レジン ツアー ブラック ゴルフボールは、更にソフトになったR...</td>\n",
       "      <td>http://store.nike.com/jp/ja_jp/pd/%25E3%2583%2...</td>\n",
       "      <td>http://usporamap287.am.adsint.biz/zoomimages/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Own eCom</td>\n",
       "      <td>AC3877-005</td>\n",
       "      <td>AC3877</td>\n",
       "      <td>ナイキ ATG スピード ジャンプ ロープ</td>\n",
       "      <td>Performance</td>\n",
       "      <td>Training</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Sport Equipment</td>\n",
       "      <td>...</td>\n",
       "      <td>28.188</td>\n",
       "      <td>28.188</td>\n",
       "      <td>28.188</td>\n",
       "      <td>EUR</td>\n",
       "      <td>2/15/2016</td>\n",
       "      <td>Not discounted yet</td>\n",
       "      <td>7/19/2016</td>\n",
       "      <td>軽く、速く、カスタマイズも可能。 ナイキ ATG スピード ジャンプ ロープは、滑りにくいボ...</td>\n",
       "      <td>http://store.nike.com/jp/ja_jp/pd/%25E3%2583%2...</td>\n",
       "      <td>http://usporamap287.am.adsint.biz/zoomimages/1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  COMPANY COUNTRY DISTRIBUTOR  ARTICLE ID MODEL NUMBER  \\\n",
       "0    Nike   Japan    Own eCom  BG0387-001       BG0387   \n",
       "1    Nike   Japan    Own eCom  839240-001       839240   \n",
       "2    Nike   Japan    Own eCom  GL0783-101       GL0783   \n",
       "3    Nike   Japan    Own eCom  GL0781-101       GL0781   \n",
       "4    Nike   Japan    Own eCom  AC3877-005       AC3877   \n",
       "\n",
       "              ARTICLE NAME        SUBBRAND SPORTS CATEGORY PRODUCT DIVISION  \\\n",
       "0     ナイキ コア ハーフ KV ゴルフバッグ     Performance            Golf      Accessories   \n",
       "1     ナイキ コルテッツ QS キッズシューズ  Sport Inspired       Lifestyle         Footwear   \n",
       "2  ナイキ レジン スピード レッド ゴルフボール     Performance            Golf      Accessories   \n",
       "3  ナイキ レジン ツアー ブラック ゴルフボール     Performance            Golf      Accessories   \n",
       "4    ナイキ ATG スピード ジャンプ ロープ     Performance        Training      Accessories   \n",
       "\n",
       "             PRODUCT GROUP                        ...                          \\\n",
       "0                     Bags                        ...                           \n",
       "1  Sport Inspired Footwear                        ...                           \n",
       "2          Sport Equipment                        ...                           \n",
       "3          Sport Equipment                        ...                           \n",
       "4          Sport Equipment                        ...                           \n",
       "\n",
       "  CURRENT PRICE IN EUR INITIAL PRICE IN SELECTED CURRENCY  \\\n",
       "0               99.144                            116.640   \n",
       "1               62.820                             82.620   \n",
       "2               31.104                             31.104   \n",
       "3               28.764                             58.320   \n",
       "4               28.188                             28.188   \n",
       "\n",
       "  CURRENT PRICE IN SELECTED CURRENCY SELECTED CURRENCY  \\\n",
       "0                             99.144               EUR   \n",
       "1                             62.820               EUR   \n",
       "2                             31.104               EUR   \n",
       "3                             28.764               EUR   \n",
       "4                             28.188               EUR   \n",
       "\n",
       "  PRODUCT INTRODUCTION DATE    DISCOUNTED SINCE PRODUCT EXIT DATE  \\\n",
       "0                 2/22/2016          12/26/2016          1/2/2017   \n",
       "1                 2/22/2016            4/5/2016         7/26/2016   \n",
       "2                 2/15/2016  Not discounted yet         8/29/2016   \n",
       "3                 2/15/2016           2/27/2017         6/27/2017   \n",
       "4                 2/15/2016  Not discounted yet         7/19/2016   \n",
       "\n",
       "                                 PRODUCT DESCRIPTION  \\\n",
       "0  整理しやすいゴルフバッグ。快適な持ち運び。 ナイキ コア ハーフ KV ゴルフバッグは、専用...   \n",
       "1  高級感のあるレトロスタイル   ナイキ コルテッツ QS キッズシューズは、上質なレザーのア...   \n",
       "2  高初速でより遠くへ ナイキ レジン スピード レッド ゴルフボールは、更にソフトになった新開...   \n",
       "3  低スピンでより遠くへ ナイキ レジン ツアー ブラック ゴルフボールは、更にソフトになったR...   \n",
       "4  軽く、速く、カスタマイズも可能。 ナイキ ATG スピード ジャンプ ロープは、滑りにくいボ...   \n",
       "\n",
       "                                         PRODUCT URL  \\\n",
       "0  http://store.nike.com/jp/ja_jp/pd/%25E3%2583%2...   \n",
       "1  http://store.nike.com/jp/ja_jp/pd/%25E3%2583%2...   \n",
       "2  http://store.nike.com/jp/ja_jp/pd/%25E3%2583%2...   \n",
       "3  http://store.nike.com/jp/ja_jp/pd/%25E3%2583%2...   \n",
       "4  http://store.nike.com/jp/ja_jp/pd/%25E3%2583%2...   \n",
       "\n",
       "                                    IMAGE-SERVER URL  \n",
       "0  http://usporamap287.am.adsint.biz/zoomimages/1...  \n",
       "1  http://usporamap287.am.adsint.biz/zoomimages/1...  \n",
       "2  http://usporamap287.am.adsint.biz/zoomimages/1...  \n",
       "3  http://usporamap287.am.adsint.biz/zoomimages/1...  \n",
       "4  http://usporamap287.am.adsint.biz/zoomimages/1...  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Japan_Not_Encoded.csv\", encoding='utf-8')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_req = data.loc[:,[\"COMPANY\", \"COUNTRY\", \"ARTICLE NAME\",\"SUBBRAND\", \"PRODUCT DESCRIPTION\", \"PRODUCT URL\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_req = data_req.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_req[\"description\"]  = data_req[\"COMPANY\"] +\" \" + data_req[\"ARTICLE NAME\"] +\" \" + data_req[\"PRODUCT DESCRIPTION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_req = data_req.drop([\"COUNTRY\", \"ARTICLE NAME\", \"PRODUCT DESCRIPTION\", \"PRODUCT URL\"], axis = 1)\n",
    "data_req.columns = [\"COMPANY\", \"subBrand\", \"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_req.description = data_req.description.fillna(\"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_req[\"subBrand\"] = data_req[\"subBrand\"].str.lower()\n",
    "data_req.description = data_req.description.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(data_req[\"subBrand\"].fillna(\"unknown\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\"description\", \"COMPANY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "performance       33771\n",
       "sport inspired    27764\n",
       "Name: subBrand, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_req[\"subBrand\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(data_req[X], y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿|¡§£₤‘’])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = [tokenize(x) for x in xtrain.description]\n",
    "texts_valid = [tokenize(x) for x in xvalid.description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tinysegmenter\n",
    "segmenter = tinysegmenter.TinySegmenter()\n",
    "tokenized_text_train = [segmenter.tokenize(x) for x in texts_train]\n",
    "tokenized_text_valid = [segmenter.tokenize(x) for x in texts_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_token_train = [' '.join(x) for x in tokenized_text_train]\n",
    "joined_token_valid = [' '.join(x) for x in tokenized_text_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 300\n",
    "\n",
    "token.fit_on_texts(joined_token_train + joined_token_valid)\n",
    "xtrain_seq = token.texts_to_sequences(joined_token_train)\n",
    "xvalid_seq = token.texts_to_sequences(joined_token_valid)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_keras_words = list(token.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "keras_tokenised_words = [text_to_word_sequence(x, lower=False) for x in (joined_token_train + joined_token_valid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word_model = gensim.models.Word2Vec(keras_tokenised_words, size=300, min_count=1, window=5, iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 36954/36954 [00:00<00:00, 143920.21it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = word_model.wv[word]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#sgd = optimizers.SGD(lr=0.00001, decay=1e-5, momentum=0.8, nesterov=True)\n",
    "rmsprop = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer= rmsprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55381 samples, validate on 6154 samples\n",
      "Epoch 1/2\n",
      "55381/55381 [==============================] - ETA: 18:43 - loss: 0.71 - ETA: 15:25 - loss: 0.72 - ETA: 14:07 - loss: 0.73 - ETA: 13:34 - loss: 0.73 - ETA: 13:19 - loss: 0.72 - ETA: 13:04 - loss: 0.72 - ETA: 12:48 - loss: 0.72 - ETA: 12:43 - loss: 0.72 - ETA: 12:50 - loss: 0.72 - ETA: 12:41 - loss: 0.71 - ETA: 12:33 - loss: 0.71 - ETA: 12:31 - loss: 0.71 - ETA: 12:26 - loss: 0.71 - ETA: 12:19 - loss: 0.71 - ETA: 12:17 - loss: 0.71 - ETA: 12:16 - loss: 0.71 - ETA: 12:13 - loss: 0.71 - ETA: 12:08 - loss: 0.70 - ETA: 12:07 - loss: 0.70 - ETA: 12:05 - loss: 0.70 - ETA: 12:01 - loss: 0.70 - ETA: 12:04 - loss: 0.70 - ETA: 12:06 - loss: 0.70 - ETA: 12:05 - loss: 0.70 - ETA: 12:03 - loss: 0.70 - ETA: 12:01 - loss: 0.70 - ETA: 12:02 - loss: 0.70 - ETA: 12:01 - loss: 0.70 - ETA: 11:57 - loss: 0.70 - ETA: 11:57 - loss: 0.70 - ETA: 11:55 - loss: 0.70 - ETA: 11:53 - loss: 0.70 - ETA: 11:51 - loss: 0.70 - ETA: 11:49 - loss: 0.70 - ETA: 11:45 - loss: 0.70 - ETA: 11:41 - loss: 0.70 - ETA: 11:40 - loss: 0.70 - ETA: 11:40 - loss: 0.70 - ETA: 11:36 - loss: 0.70 - ETA: 11:33 - loss: 0.70 - ETA: 11:30 - loss: 0.70 - ETA: 11:26 - loss: 0.70 - ETA: 11:24 - loss: 0.70 - ETA: 11:22 - loss: 0.70 - ETA: 11:21 - loss: 0.70 - ETA: 11:18 - loss: 0.69 - ETA: 11:15 - loss: 0.69 - ETA: 11:13 - loss: 0.69 - ETA: 11:10 - loss: 0.69 - ETA: 11:07 - loss: 0.69 - ETA: 11:04 - loss: 0.69 - ETA: 11:02 - loss: 0.69 - ETA: 10:58 - loss: 0.69 - ETA: 10:55 - loss: 0.69 - ETA: 10:52 - loss: 0.69 - ETA: 10:50 - loss: 0.69 - ETA: 10:47 - loss: 0.69 - ETA: 10:44 - loss: 0.69 - ETA: 10:42 - loss: 0.69 - ETA: 10:40 - loss: 0.69 - ETA: 10:37 - loss: 0.69 - ETA: 10:33 - loss: 0.69 - ETA: 10:31 - loss: 0.69 - ETA: 10:27 - loss: 0.69 - ETA: 10:23 - loss: 0.69 - ETA: 10:19 - loss: 0.69 - ETA: 10:16 - loss: 0.69 - ETA: 10:12 - loss: 0.69 - ETA: 10:08 - loss: 0.69 - ETA: 10:04 - loss: 0.69 - ETA: 10:00 - loss: 0.69 - ETA: 9:55 - loss: 0.6940 - ETA: 9:52 - loss: 0.693 - ETA: 9:48 - loss: 0.693 - ETA: 9:44 - loss: 0.692 - ETA: 9:40 - loss: 0.692 - ETA: 9:36 - loss: 0.691 - ETA: 9:32 - loss: 0.691 - ETA: 9:28 - loss: 0.690 - ETA: 9:25 - loss: 0.690 - ETA: 9:21 - loss: 0.689 - ETA: 9:17 - loss: 0.689 - ETA: 9:13 - loss: 0.689 - ETA: 9:09 - loss: 0.689 - ETA: 9:06 - loss: 0.689 - ETA: 9:02 - loss: 0.688 - ETA: 8:58 - loss: 0.688 - ETA: 8:54 - loss: 0.688 - ETA: 8:50 - loss: 0.687 - ETA: 8:46 - loss: 0.687 - ETA: 8:42 - loss: 0.686 - ETA: 8:38 - loss: 0.686 - ETA: 8:34 - loss: 0.686 - ETA: 8:30 - loss: 0.685 - ETA: 8:26 - loss: 0.685 - ETA: 8:23 - loss: 0.684 - ETA: 8:19 - loss: 0.684 - ETA: 8:15 - loss: 0.683 - ETA: 8:11 - loss: 0.683 - ETA: 8:07 - loss: 0.682 - ETA: 8:03 - loss: 0.682 - ETA: 7:59 - loss: 0.682 - ETA: 7:55 - loss: 0.682 - ETA: 7:51 - loss: 0.682 - ETA: 7:47 - loss: 0.681 - ETA: 7:43 - loss: 0.681 - ETA: 7:39 - loss: 0.681 - ETA: 7:35 - loss: 0.681 - ETA: 7:31 - loss: 0.680 - ETA: 7:27 - loss: 0.680 - ETA: 7:23 - loss: 0.679 - ETA: 7:19 - loss: 0.678 - ETA: 7:16 - loss: 0.678 - ETA: 7:12 - loss: 0.677 - ETA: 7:08 - loss: 0.676 - ETA: 7:04 - loss: 0.676 - ETA: 7:00 - loss: 0.676 - ETA: 6:56 - loss: 0.676 - ETA: 6:52 - loss: 0.675 - ETA: 6:48 - loss: 0.675 - ETA: 6:44 - loss: 0.674 - ETA: 6:40 - loss: 0.674 - ETA: 6:37 - loss: 0.673 - ETA: 6:33 - loss: 0.673 - ETA: 6:29 - loss: 0.672 - ETA: 6:25 - loss: 0.671 - ETA: 6:21 - loss: 0.671 - ETA: 6:17 - loss: 0.670 - ETA: 6:13 - loss: 0.670 - ETA: 6:09 - loss: 0.670 - ETA: 6:05 - loss: 0.669 - ETA: 6:01 - loss: 0.669 - ETA: 5:57 - loss: 0.668 - ETA: 5:53 - loss: 0.667 - ETA: 5:49 - loss: 0.667 - ETA: 5:45 - loss: 0.667 - ETA: 5:41 - loss: 0.666 - ETA: 5:37 - loss: 0.666 - ETA: 5:33 - loss: 0.665 - ETA: 5:30 - loss: 0.665 - ETA: 5:26 - loss: 0.664 - ETA: 5:22 - loss: 0.664 - ETA: 5:18 - loss: 0.664 - ETA: 5:13 - loss: 0.663 - ETA: 5:09 - loss: 0.663 - ETA: 5:05 - loss: 0.662 - ETA: 5:01 - loss: 0.661 - ETA: 4:57 - loss: 0.661 - ETA: 4:53 - loss: 0.660 - ETA: 4:49 - loss: 0.659 - ETA: 4:44 - loss: 0.659 - ETA: 4:40 - loss: 0.658 - ETA: 4:36 - loss: 0.658 - ETA: 4:32 - loss: 0.657 - ETA: 4:28 - loss: 0.657 - ETA: 4:24 - loss: 0.656 - ETA: 4:20 - loss: 0.655 - ETA: 4:15 - loss: 0.655 - ETA: 4:11 - loss: 0.655 - ETA: 4:07 - loss: 0.654 - ETA: 4:03 - loss: 0.654 - ETA: 3:59 - loss: 0.653 - ETA: 3:54 - loss: 0.653 - ETA: 3:50 - loss: 0.652 - ETA: 3:46 - loss: 0.652 - ETA: 3:42 - loss: 0.651 - ETA: 3:37 - loss: 0.651 - ETA: 3:33 - loss: 0.650 - ETA: 3:29 - loss: 0.650 - ETA: 3:25 - loss: 0.649 - ETA: 3:20 - loss: 0.649 - ETA: 3:16 - loss: 0.648 - ETA: 3:12 - loss: 0.648 - ETA: 3:08 - loss: 0.647 - ETA: 3:03 - loss: 0.647 - ETA: 2:59 - loss: 0.646 - ETA: 2:55 - loss: 0.646 - ETA: 2:50 - loss: 0.646 - ETA: 2:46 - loss: 0.645 - ETA: 2:42 - loss: 0.645 - ETA: 2:37 - loss: 0.645 - ETA: 2:33 - loss: 0.644 - ETA: 2:28 - loss: 0.644 - ETA: 2:24 - loss: 0.643 - ETA: 2:20 - loss: 0.643 - ETA: 2:15 - loss: 0.642 - ETA: 2:11 - loss: 0.642 - ETA: 2:07 - loss: 0.641 - ETA: 2:02 - loss: 0.641 - ETA: 1:58 - loss: 0.640 - ETA: 1:53 - loss: 0.640 - ETA: 1:49 - loss: 0.640 - ETA: 1:44 - loss: 0.639 - ETA: 1:40 - loss: 0.639 - ETA: 1:36 - loss: 0.638 - ETA: 1:31 - loss: 0.638 - ETA: 1:27 - loss: 0.637 - ETA: 1:22 - loss: 0.636 - ETA: 1:18 - loss: 0.636 - ETA: 1:13 - loss: 0.635 - ETA: 1:09 - loss: 0.635 - ETA: 1:04 - loss: 0.634 - ETA: 1:00 - loss: 0.634 - ETA: 55s - loss: 0.633 - ETA: 51s - loss: 0.63 - ETA: 46s - loss: 0.63 - ETA: 42s - loss: 0.63 - ETA: 37s - loss: 0.63 - ETA: 33s - loss: 0.63 - ETA: 28s - loss: 0.63 - ETA: 24s - loss: 0.63 - ETA: 19s - loss: 0.63 - ETA: 15s - loss: 0.63 - ETA: 10s - loss: 0.62 - ETA: 6s - loss: 0.6291 - ETA: 1s - loss: 0.628 - 1024s 18ms/step - loss: 0.6287 - val_loss: 0.5059\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55381/55381 [==============================] - ETA: 17:38 - loss: 0.49 - ETA: 17:49 - loss: 0.49 - ETA: 17:54 - loss: 0.51 - ETA: 17:49 - loss: 0.52 - ETA: 17:42 - loss: 0.52 - ETA: 18:00 - loss: 0.51 - ETA: 18:00 - loss: 0.51 - ETA: 17:52 - loss: 0.51 - ETA: 17:43 - loss: 0.51 - ETA: 17:41 - loss: 0.51 - ETA: 17:33 - loss: 0.51 - ETA: 17:27 - loss: 0.51 - ETA: 17:25 - loss: 0.51 - ETA: 17:22 - loss: 0.51 - ETA: 17:15 - loss: 0.52 - ETA: 17:08 - loss: 0.52 - ETA: 17:04 - loss: 0.52 - ETA: 16:57 - loss: 0.52 - ETA: 16:51 - loss: 0.52 - ETA: 16:48 - loss: 0.52 - ETA: 16:45 - loss: 0.52 - ETA: 16:40 - loss: 0.52 - ETA: 16:35 - loss: 0.52 - ETA: 16:32 - loss: 0.52 - ETA: 16:26 - loss: 0.52 - ETA: 16:18 - loss: 0.52 - ETA: 16:13 - loss: 0.52 - ETA: 16:09 - loss: 0.52 - ETA: 16:03 - loss: 0.52 - ETA: 15:57 - loss: 0.52 - ETA: 15:55 - loss: 0.52 - ETA: 15:51 - loss: 0.52 - ETA: 15:45 - loss: 0.52 - ETA: 15:40 - loss: 0.52 - ETA: 15:37 - loss: 0.52 - ETA: 15:32 - loss: 0.52 - ETA: 15:25 - loss: 0.52 - ETA: 15:21 - loss: 0.52 - ETA: 15:16 - loss: 0.52 - ETA: 15:11 - loss: 0.52 - ETA: 15:06 - loss: 0.52 - ETA: 15:02 - loss: 0.52 - ETA: 14:57 - loss: 0.52 - ETA: 14:53 - loss: 0.52 - ETA: 14:49 - loss: 0.52 - ETA: 14:45 - loss: 0.52 - ETA: 14:41 - loss: 0.52 - ETA: 14:37 - loss: 0.52 - ETA: 14:32 - loss: 0.52 - ETA: 14:27 - loss: 0.52 - ETA: 14:21 - loss: 0.52 - ETA: 14:15 - loss: 0.52 - ETA: 14:11 - loss: 0.52 - ETA: 14:06 - loss: 0.52 - ETA: 14:01 - loss: 0.52 - ETA: 13:56 - loss: 0.52 - ETA: 13:52 - loss: 0.52 - ETA: 13:46 - loss: 0.52 - ETA: 13:41 - loss: 0.52 - ETA: 13:37 - loss: 0.52 - ETA: 13:31 - loss: 0.52 - ETA: 13:26 - loss: 0.51 - ETA: 13:21 - loss: 0.51 - ETA: 13:16 - loss: 0.51 - ETA: 13:11 - loss: 0.51 - ETA: 13:06 - loss: 0.51 - ETA: 13:01 - loss: 0.51 - ETA: 12:56 - loss: 0.51 - ETA: 12:51 - loss: 0.51 - ETA: 12:47 - loss: 0.51 - ETA: 12:42 - loss: 0.51 - ETA: 12:37 - loss: 0.51 - ETA: 12:33 - loss: 0.51 - ETA: 12:28 - loss: 0.51 - ETA: 12:24 - loss: 0.51 - ETA: 12:18 - loss: 0.51 - ETA: 12:13 - loss: 0.51 - ETA: 12:08 - loss: 0.51 - ETA: 12:03 - loss: 0.51 - ETA: 11:58 - loss: 0.51 - ETA: 11:53 - loss: 0.51 - ETA: 11:49 - loss: 0.51 - ETA: 11:43 - loss: 0.51 - ETA: 11:38 - loss: 0.51 - ETA: 11:33 - loss: 0.51 - ETA: 11:27 - loss: 0.51 - ETA: 11:22 - loss: 0.51 - ETA: 11:17 - loss: 0.51 - ETA: 11:13 - loss: 0.51 - ETA: 11:07 - loss: 0.51 - ETA: 11:02 - loss: 0.51 - ETA: 10:57 - loss: 0.51 - ETA: 10:52 - loss: 0.51 - ETA: 10:46 - loss: 0.51 - ETA: 10:41 - loss: 0.51 - ETA: 10:36 - loss: 0.51 - ETA: 10:31 - loss: 0.51 - ETA: 10:26 - loss: 0.51 - ETA: 10:21 - loss: 0.51 - ETA: 10:16 - loss: 0.50 - ETA: 10:11 - loss: 0.51 - ETA: 10:11 - loss: 0.51 - ETA: 10:15 - loss: 0.51 - ETA: 10:12 - loss: 0.50 - ETA: 10:08 - loss: 0.50 - ETA: 10:04 - loss: 0.50 - ETA: 9:59 - loss: 0.5081 - ETA: 9:53 - loss: 0.508 - ETA: 9:48 - loss: 0.508 - ETA: 9:43 - loss: 0.508 - ETA: 9:38 - loss: 0.507 - ETA: 9:32 - loss: 0.507 - ETA: 9:27 - loss: 0.507 - ETA: 9:22 - loss: 0.507 - ETA: 9:16 - loss: 0.506 - ETA: 9:11 - loss: 0.506 - ETA: 9:06 - loss: 0.506 - ETA: 9:00 - loss: 0.506 - ETA: 8:55 - loss: 0.505 - ETA: 8:49 - loss: 0.505 - ETA: 8:44 - loss: 0.504 - ETA: 8:38 - loss: 0.504 - ETA: 8:33 - loss: 0.504 - ETA: 8:28 - loss: 0.505 - ETA: 8:23 - loss: 0.505 - ETA: 8:18 - loss: 0.504 - ETA: 8:13 - loss: 0.503 - ETA: 8:07 - loss: 0.503 - ETA: 8:02 - loss: 0.503 - ETA: 7:57 - loss: 0.503 - ETA: 7:52 - loss: 0.503 - ETA: 7:48 - loss: 0.503 - ETA: 7:44 - loss: 0.503 - ETA: 7:39 - loss: 0.503 - ETA: 7:34 - loss: 0.503 - ETA: 7:30 - loss: 0.502 - ETA: 7:24 - loss: 0.503 - ETA: 7:19 - loss: 0.503 - ETA: 7:15 - loss: 0.502 - ETA: 7:10 - loss: 0.502 - ETA: 7:04 - loss: 0.502 - ETA: 6:59 - loss: 0.502 - ETA: 6:54 - loss: 0.502 - ETA: 6:48 - loss: 0.502 - ETA: 6:43 - loss: 0.501 - ETA: 6:38 - loss: 0.501 - ETA: 6:33 - loss: 0.501 - ETA: 6:27 - loss: 0.501 - ETA: 6:22 - loss: 0.501 - ETA: 6:16 - loss: 0.501 - ETA: 6:11 - loss: 0.501 - ETA: 6:05 - loss: 0.500 - ETA: 6:00 - loss: 0.500 - ETA: 5:55 - loss: 0.500 - ETA: 5:50 - loss: 0.499 - ETA: 5:44 - loss: 0.499 - ETA: 5:38 - loss: 0.499 - ETA: 5:33 - loss: 0.499 - ETA: 5:27 - loss: 0.499 - ETA: 5:21 - loss: 0.498 - ETA: 5:16 - loss: 0.498 - ETA: 5:10 - loss: 0.498 - ETA: 5:05 - loss: 0.498 - ETA: 4:59 - loss: 0.498 - ETA: 4:53 - loss: 0.498 - ETA: 4:48 - loss: 0.498 - ETA: 4:42 - loss: 0.498 - ETA: 4:36 - loss: 0.499 - ETA: 4:31 - loss: 0.499 - ETA: 4:25 - loss: 0.498 - ETA: 4:19 - loss: 0.498 - ETA: 4:14 - loss: 0.498 - ETA: 4:08 - loss: 0.498 - ETA: 4:03 - loss: 0.498 - ETA: 3:57 - loss: 0.497 - ETA: 3:51 - loss: 0.497 - ETA: 3:46 - loss: 0.497 - ETA: 3:40 - loss: 0.497 - ETA: 3:34 - loss: 0.497 - ETA: 3:28 - loss: 0.496 - ETA: 3:23 - loss: 0.497 - ETA: 3:17 - loss: 0.496 - ETA: 3:12 - loss: 0.496 - ETA: 3:06 - loss: 0.496 - ETA: 3:00 - loss: 0.496 - ETA: 2:54 - loss: 0.495 - ETA: 2:49 - loss: 0.495 - ETA: 2:43 - loss: 0.495 - ETA: 2:37 - loss: 0.495 - ETA: 2:31 - loss: 0.495 - ETA: 2:26 - loss: 0.494 - ETA: 2:20 - loss: 0.494 - ETA: 2:14 - loss: 0.494 - ETA: 2:08 - loss: 0.494 - ETA: 2:03 - loss: 0.494 - ETA: 1:57 - loss: 0.494 - ETA: 1:51 - loss: 0.494 - ETA: 1:45 - loss: 0.494 - ETA: 1:40 - loss: 0.494 - ETA: 1:34 - loss: 0.494 - ETA: 1:28 - loss: 0.494 - ETA: 1:22 - loss: 0.493 - ETA: 1:17 - loss: 0.493 - ETA: 1:11 - loss: 0.493 - ETA: 1:05 - loss: 0.493 - ETA: 59s - loss: 0.493 - ETA: 53s - loss: 0.49 - ETA: 48s - loss: 0.49 - ETA: 42s - loss: 0.49 - ETA: 36s - loss: 0.49 - ETA: 30s - loss: 0.49 - ETA: 25s - loss: 0.49 - ETA: 19s - loss: 0.49 - ETA: 13s - loss: 0.49 - ETA: 7s - loss: 0.4915 - ETA: 1s - loss: 0.491 - 1292s 23ms/step - loss: 0.4911 - val_loss: 0.4245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x148c62e3518>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=256, epochs=2, verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(xvalid_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_label = [np.argmax(i) for i in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\narendran.thesma\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "predict_label = lbl_enc.inverse_transform(predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\narendran.thesma\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "yvalid_label = lbl_enc.inverse_transform(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'description' : xvalid.description, 'actuals' : yvalid_label, 'predictions' : predict_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8132437456004686"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_df.actuals, test_df.predictions,average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.815729606759831"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_df.actuals, test_df.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
